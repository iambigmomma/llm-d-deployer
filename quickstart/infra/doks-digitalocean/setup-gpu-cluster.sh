#!/usr/bin/env bash
# -*- indent-tabs-mode: nil; tab-width: 4; sh-indentation: 4; -*-

set -euo pipefail

### GLOBALS ###
SCRIPT_NAME="$(basename "$0")"
NVIDIA_DEVICE_PLUGIN_NAMESPACE="nvidia-device-plugin"
NVIDIA_DEVICE_PLUGIN_VERSION="v0.17.1"
KUBERNETES_CONTEXT=""
FORCE_REINSTALL=false
DRY_RUN=false

# ANSI color helpers
COLOR_RESET=$'\e[0m'
COLOR_GREEN=$'\e[32m'
COLOR_YELLOW=$'\e[33m'
COLOR_RED=$'\e[31m'
COLOR_BLUE=$'\e[34m'

log_info() {
    echo "${COLOR_BLUE}â„¹ï¸  $*${COLOR_RESET}"
}

log_success() {
    echo "${COLOR_GREEN}âœ… $*${COLOR_RESET}"
}

log_warning() {
    echo "${COLOR_YELLOW}âš ï¸  $*${COLOR_RESET}"
}

log_error() {
    echo "${COLOR_RED}âŒ $*${COLOR_RESET}" >&2
}

die() {
    log_error "$*"
    exit 1
}

### HELP ###
print_help() {
    cat <<EOF
Usage: ${SCRIPT_NAME} [OPTIONS]

DigitalOcean GPU Kubernetes Cluster é å®‰è£è…³æœ¬
æ­¤è…³æœ¬æœƒå®‰è£ä¸¦é…ç½® NVIDIA Device Pluginï¼Œä»¥åŠä¿®å¾© GPU ç¯€é»æ¨™ç±¤å•é¡Œï¼Œ
ç¢ºä¿å¾ŒçºŒçš„ llmd-installer.sh å¯ä»¥æ­£å¸¸é‹è¡Œã€‚

Options:
  -g, --context PATH           æŒ‡å®š Kubernetes context æ–‡ä»¶è·¯å¾„
  -f, --force-reinstall        å¼·åˆ¶é‡æ–°å®‰è£ NVIDIA Device Plugin
  -d, --dry-run               åªé¡¯ç¤ºå°‡è¦åŸ·è¡Œçš„æ“ä½œï¼Œä¸å¯¦éš›åŸ·è¡Œ
  -h, --help                  é¡¯ç¤ºæ­¤å¹«åŠ©ä¿¡æ¯ä¸¦é€€å‡º

Examples:
  ${SCRIPT_NAME}                           # ä½¿ç”¨ç•¶å‰ kubectl context
  ${SCRIPT_NAME} -g ~/.kube/config         # ä½¿ç”¨æŒ‡å®šçš„ kubeconfig
  ${SCRIPT_NAME} --force-reinstall         # å¼·åˆ¶é‡æ–°å®‰è£
  ${SCRIPT_NAME} --dry-run                 # é è¦½å°‡è¦åŸ·è¡Œçš„æ“ä½œ

EOF
}

### ARGUMENT PARSING ###
parse_args() {
    while [[ $# -gt 0 ]]; do
        case "$1" in
            -g|--context)
                KUBERNETES_CONTEXT="$2"
                shift 2
                ;;
            -f|--force-reinstall)
                FORCE_REINSTALL=true
                shift
                ;;
            -d|--dry-run)
                DRY_RUN=true
                shift
                ;;
            -h|--help)
                print_help
                exit 0
                ;;
            *)
                die "æœªçŸ¥é¸é …: $1"
                ;;
        esac
    done
}

### UTILITIES ###
check_cmd() {
    command -v "$1" &>/dev/null || die "ç¼ºå°‘å¿…è¦å‘½ä»¤: $1"
}

check_dependencies() {
    local required_cmds=(kubectl helm)
    for cmd in "${required_cmds[@]}"; do
        check_cmd "$cmd"
    done
}

setup_kubectl() {
    if [[ -n "${KUBERNETES_CONTEXT}" ]]; then
        if [[ ! -f "${KUBERNETES_CONTEXT}" ]]; then
            die "æŒ‡å®šçš„ kubeconfig æ–‡ä»¶ä¸å­˜åœ¨: ${KUBERNETES_CONTEXT}"
        fi
        KCMD="kubectl --kubeconfig ${KUBERNETES_CONTEXT}"
        HCMD="helm --kubeconfig ${KUBERNETES_CONTEXT}"
        log_info "ä½¿ç”¨æŒ‡å®šçš„ kubeconfig: ${KUBERNETES_CONTEXT}"
    else
        KCMD="kubectl"
        HCMD="helm"
        log_info "ä½¿ç”¨ç•¶å‰çš„ kubectl context"
    fi
}

check_cluster_reachability() {
    log_info "æª¢æŸ¥ Kubernetes é›†ç¾¤é€£æ¥æ€§..."
    if ${KCMD} cluster-info &>/dev/null; then
        log_success "æˆåŠŸé€£æ¥åˆ° Kubernetes é›†ç¾¤"
    else
        die "ç„¡æ³•é€£æ¥åˆ° Kubernetes é›†ç¾¤ï¼Œè«‹æª¢æŸ¥æ‚¨çš„ kubectl é…ç½®"
    fi
}

verify_digitalocean_gpu_cluster() {
    log_info "é©—è­‰é€™æ˜¯ DigitalOcean GPU é›†ç¾¤..."
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ GPU ç¯€é»
    local gpu_nodes
    gpu_nodes=$(${KCMD} get nodes -l doks.digitalocean.com/gpu-brand=nvidia --no-headers 2>/dev/null | wc -l || echo "0")
    
    if [[ "${gpu_nodes}" -eq 0 ]]; then
        log_warning "æœªæ‰¾åˆ° DigitalOcean GPU ç¯€é»æ¨™ç±¤ï¼Œæª¢æŸ¥ç¯€é»é¡å‹..."
        # æª¢æŸ¥æ˜¯å¦æœ‰ GPU å¯¦ä¾‹é¡å‹çš„ç¯€é»
        gpu_nodes=$(${KCMD} get nodes -o jsonpath='{.items[*].metadata.labels.node\.kubernetes\.io/instance-type}' | grep -c "gpu-" || echo "0")
        
        if [[ "${gpu_nodes}" -eq 0 ]]; then
            die "æ­¤é›†ç¾¤ä¼¼ä¹æ²’æœ‰ GPU ç¯€é»ã€‚è«‹ç¢ºèªæ‚¨ä½¿ç”¨çš„æ˜¯ DigitalOcean GPU é›†ç¾¤ã€‚"
        fi
    fi
    
    log_success "æª¢æ¸¬åˆ° ${gpu_nodes} å€‹ GPU ç¯€é»"
}

get_gpu_nodes() {
    # å„ªå…ˆä½¿ç”¨ DigitalOcean æ¨™ç±¤
    local gpu_nodes
    gpu_nodes=$(${KCMD} get nodes -l doks.digitalocean.com/gpu-brand=nvidia -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
    
    # å¦‚æœæ²’æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨å¯¦ä¾‹é¡å‹
    if [[ -z "${gpu_nodes}" ]]; then
        gpu_nodes=$(${KCMD} get nodes -o json | jq -r '.items[] | select(.metadata.labels."node.kubernetes.io/instance-type" | test("gpu-")) | .metadata.name' 2>/dev/null || echo "")
    fi
    
    echo "${gpu_nodes}"
}

install_nvidia_device_plugin() {
    log_info "å®‰è£ NVIDIA Device Plugin..."
    
    # æª¢æŸ¥æ˜¯å¦å·²å®‰è£
    if ${KCMD} get namespace "${NVIDIA_DEVICE_PLUGIN_NAMESPACE}" &>/dev/null && [[ "${FORCE_REINSTALL}" != "true" ]]; then
        log_warning "NVIDIA Device Plugin å·²å®‰è£ï¼Œä½¿ç”¨ --force-reinstall å¼·åˆ¶é‡æ–°å®‰è£"
        return 0
    fi
    
    if [[ "${DRY_RUN}" == "true" ]]; then
        log_info "[DRY RUN] å°‡æœƒå®‰è£ NVIDIA Device Plugin ${NVIDIA_DEVICE_PLUGIN_VERSION}"
        return 0
    fi
    
    # å¦‚æœéœ€è¦é‡æ–°å®‰è£ï¼Œå…ˆåˆªé™¤
    if [[ "${FORCE_REINSTALL}" == "true" ]]; then
        log_info "ç§»é™¤ç¾æœ‰çš„ NVIDIA Device Plugin..."
        ${HCMD} uninstall nvdp -n "${NVIDIA_DEVICE_PLUGIN_NAMESPACE}" --ignore-not-found || true
        ${KCMD} delete namespace "${NVIDIA_DEVICE_PLUGIN_NAMESPACE}" --ignore-not-found || true
        sleep 5
    fi
    
    # å‰µå»º namespace
    ${KCMD} create namespace "${NVIDIA_DEVICE_PLUGIN_NAMESPACE}" --dry-run=client -o yaml | ${KCMD} apply -f -
    
    # æ·»åŠ  NVIDIA Helm repository
    ${HCMD} repo add nvdp https://nvidia.github.io/k8s-device-plugin
    ${HCMD} repo update
    
    # å®‰è£ NVIDIA Device Plugin
    ${HCMD} install nvdp nvdp/nvidia-device-plugin \
        --namespace "${NVIDIA_DEVICE_PLUGIN_NAMESPACE}" \
        --version "${NVIDIA_DEVICE_PLUGIN_VERSION}" \
        --set nodeSelector="nvidia.com/gpu=true" \
        --wait
    
    log_success "NVIDIA Device Plugin å®‰è£å®Œæˆ"
}

wait_for_device_plugin() {
    log_info "ç­‰å¾… NVIDIA Device Plugin å°±ç·’..."
    
    if [[ "${DRY_RUN}" == "true" ]]; then
        log_info "[DRY RUN] å°‡æœƒç­‰å¾… Device Plugin pods å°±ç·’"
        return 0
    fi
    
    # ç­‰å¾… DaemonSet å°±ç·’
    local max_attempts=30
    local attempt=0
    
    while [[ ${attempt} -lt ${max_attempts} ]]; do
        local ready_pods
        ready_pods=$(${KCMD} get daemonset nvdp-nvidia-device-plugin -n "${NVIDIA_DEVICE_PLUGIN_NAMESPACE}" -o jsonpath='{.status.numberReady}' 2>/dev/null || echo "0")
        local desired_pods
        desired_pods=$(${KCMD} get daemonset nvdp-nvidia-device-plugin -n "${NVIDIA_DEVICE_PLUGIN_NAMESPACE}" -o jsonpath='{.status.desiredNumberScheduled}' 2>/dev/null || echo "0")
        
        if [[ "${ready_pods}" -gt 0 ]] && [[ "${ready_pods}" -eq "${desired_pods}" ]]; then
            log_success "NVIDIA Device Plugin DaemonSet å·²å°±ç·’ (${ready_pods}/${desired_pods})"
            return 0
        fi
        
        log_info "ç­‰å¾… Device Plugin DaemonSet å°±ç·’... (${ready_pods}/${desired_pods}) - å˜—è©¦ $((attempt + 1))/${max_attempts}"
        sleep 10
        ((attempt++))
    done
    
    log_warning "Device Plugin DaemonSet åœ¨é æœŸæ™‚é–“å…§æœªå®Œå…¨å°±ç·’ï¼Œç¹¼çºŒåŸ·è¡Œæ¨™ç±¤ä¿®å¾©..."
}

fix_gpu_node_labels() {
    log_info "ä¿®å¾© GPU ç¯€é»æ¨™ç±¤..."
    
    local gpu_nodes
    gpu_nodes=$(get_gpu_nodes)
    
    if [[ -z "${gpu_nodes}" ]]; then
        die "æœªæ‰¾åˆ° GPU ç¯€é»"
    fi
    
    for node in ${gpu_nodes}; do
        log_info "è™•ç†ç¯€é»: ${node}"
        
        if [[ "${DRY_RUN}" == "true" ]]; then
            log_info "[DRY RUN] å°‡æœƒç‚ºç¯€é» ${node} æ·»åŠ æ¨™ç±¤: feature.node.kubernetes.io/pci-10de.present=true"
            continue
        fi
        
        # æ·»åŠ å¿…è¦çš„æ¨™ç±¤
        ${KCMD} label nodes "${node}" feature.node.kubernetes.io/pci-10de.present=true --overwrite
        ${KCMD} label nodes "${node}" nvidia.com/gpu.present=true --overwrite
        
        log_success "ç¯€é» ${node} æ¨™ç±¤ä¿®å¾©å®Œæˆ"
    done
}

verify_gpu_resources() {
    log_info "é©—è­‰ GPU è³‡æºå¯ç”¨æ€§..."
    
    if [[ "${DRY_RUN}" == "true" ]]; then
        log_info "[DRY RUN] å°‡æœƒé©—è­‰ GPU è³‡æºæ˜¯å¦å¯ç”¨"
        return 0
    fi
    
    local gpu_nodes
    gpu_nodes=$(get_gpu_nodes)
    
    local total_gpus=0
    local available_gpus=0
    
    for node in ${gpu_nodes}; do
        local gpu_capacity
        gpu_capacity=$(${KCMD} get node "${node}" -o jsonpath='{.status.capacity.nvidia\.com/gpu}' 2>/dev/null || echo "0")
        local gpu_allocatable
        gpu_allocatable=$(${KCMD} get node "${node}" -o jsonpath='{.status.allocatable.nvidia\.com/gpu}' 2>/dev/null || echo "0")
        
        if [[ "${gpu_capacity}" -gt 0 ]]; then
            log_success "ç¯€é» ${node}: GPU å®¹é‡=${gpu_capacity}, å¯åˆ†é…=${gpu_allocatable}"
            total_gpus=$((total_gpus + gpu_capacity))
            available_gpus=$((available_gpus + gpu_allocatable))
        else
            log_warning "ç¯€é» ${node}: æœªæª¢æ¸¬åˆ° GPU è³‡æº"
        fi
    done
    
    if [[ ${total_gpus} -gt 0 ]]; then
        log_success "GPU è³‡æºé©—è­‰å®Œæˆ: ç¸½è¨ˆ ${total_gpus} å€‹ GPUï¼Œå¯ç”¨ ${available_gpus} å€‹"
    else
        die "æœªæª¢æ¸¬åˆ°ä»»ä½• GPU è³‡æºï¼Œè«‹æª¢æŸ¥ NVIDIA Device Plugin å®‰è£"
    fi
}

run_health_check() {
    log_info "åŸ·è¡Œå¥åº·æª¢æŸ¥..."
    
    if [[ "${DRY_RUN}" == "true" ]]; then
        log_info "[DRY RUN] å°‡æœƒåŸ·è¡Œå®Œæ•´çš„å¥åº·æª¢æŸ¥"
        return 0
    fi
    
    # æª¢æŸ¥ Device Plugin pods
    local plugin_pods
    plugin_pods=$(${KCMD} get pods -n "${NVIDIA_DEVICE_PLUGIN_NAMESPACE}" -l app.kubernetes.io/name=nvidia-device-plugin --no-headers 2>/dev/null | wc -l || echo "0")
    
    if [[ "${plugin_pods}" -gt 0 ]]; then
        log_success "NVIDIA Device Plugin pods: ${plugin_pods} å€‹æ­£åœ¨é‹è¡Œ"
    else
        log_warning "æœªæ‰¾åˆ°é‹è¡Œä¸­çš„ NVIDIA Device Plugin pods"
    fi
    
    # æª¢æŸ¥ GPU taints å’Œ tolerations
    local gpu_nodes
    gpu_nodes=$(get_gpu_nodes)
    
    for node in ${gpu_nodes}; do
        local has_gpu_taint
        has_gpu_taint=$(${KCMD} get node "${node}" -o jsonpath='{.spec.taints[?(@.key=="nvidia.com/gpu")].key}' 2>/dev/null || echo "")
        
        if [[ -n "${has_gpu_taint}" ]]; then
            log_success "ç¯€é» ${node}: å…·æœ‰æ­£ç¢ºçš„ GPU taint"
        else
            log_warning "ç¯€é» ${node}: ç¼ºå°‘ GPU taintï¼Œé€™å¯èƒ½æ˜¯æ­£å¸¸çš„"
        fi
    done
    
    log_success "å¥åº·æª¢æŸ¥å®Œæˆ"
}

### MAIN FUNCTION ###
main() {
    parse_args "$@"
    
    log_info "ğŸš€ DigitalOcean GPU Kubernetes Cluster é å®‰è£è…³æœ¬"
    log_info "ç‰ˆæœ¬: NVIDIA Device Plugin ${NVIDIA_DEVICE_PLUGIN_VERSION}"
    
    if [[ "${DRY_RUN}" == "true" ]]; then
        log_warning "DRY RUN æ¨¡å¼ - ä¸æœƒåŸ·è¡Œå¯¦éš›æ“ä½œ"
    fi
    
    check_dependencies
    setup_kubectl
    check_cluster_reachability
    verify_digitalocean_gpu_cluster
    
    install_nvidia_device_plugin
    wait_for_device_plugin
    fix_gpu_node_labels
    
    # ç­‰å¾…ä¸€ä¸‹è®“è³‡æºåˆ·æ–°
    if [[ "${DRY_RUN}" != "true" ]]; then
        log_info "ç­‰å¾…è³‡æºåˆ·æ–°..."
        sleep 30
    fi
    
    verify_gpu_resources
    run_health_check
    
    log_success "ğŸ‰ DigitalOcean GPU Cluster é å®‰è£å®Œæˆï¼"
    log_info "ç¾åœ¨æ‚¨å¯ä»¥é‹è¡Œ llmd-installer.sh é€²è¡Œ LLM-D éƒ¨ç½²"
}

main "$@" 